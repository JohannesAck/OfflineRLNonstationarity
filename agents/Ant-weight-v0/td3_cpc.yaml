env_name: Ant-weight-v0
env_name_eval: Ant-weight-v0

method: cpc

online:
  buffer_size: 100000
  
  num_train_envs: 96
  max_grad_norm: 1.0
  gamma: 0.99
  actor_network_config:
    num_hidden_layers: 2
    num_hidden_units: 256
  critic_network_config:
    num_hidden_layers: 2
    num_hidden_units: 256
  lr: 0.001
  num_train_steps: 10000000
  
  update_freq: 2
  policy_freq: 2

  polyak: 0.995  # target network polyak
  batch_size: 256  # batch size per update
  warmup_steps:  25  # times number of envs
  exploration_std: 0.1
  exploration_clip: 0.1
  policy_noise_std: 0.1  # noise during update
  policy_noise_clip: 0.1  # noise during update

  n_updates_jit: 8
  n_transitions_jit: 16
  
  num_test_rollouts: 10
  evaluate_every_epochs: 100
  video_every_epochs: 500
  
  n_deployments_record: 1000
  traj_recording_parallel: 20
  record_hip_as_state: true

vae:
  batch_size: 4096  # if using recurrent vae, this is the number of deployments sampled
  train_steps: 300000
  
  latent_dim: 4
  lr: 0.0003
  
  decoder_hidden_layers: 2
  encoder_hidden_layers: 2
  num_hidden_units: 128

  context_len_cpc: 3
  cpc_offset: 1
  cpc_trans_encoding_dim: 4

  vae_beta: 0.05

  n_updates_jit: 1  # jitting multiple updates together actually decreases performance with vrnn, for some reason
  
  evaluate_every_steps: 50000

offline:
  train_steps: 2000000
  
  buffer_size: 1000000
  batch_size: 512
  gamma: 0.99
  actor_lr: 0.001
  critic_lr: 0.001
  max_grad_norm: 1.0
  
  network_config:
    num_hidden_layers: 2
    num_hidden_units: 128
  
  polyak: 0.995
  td3_alpha: 6.5

  policy_freq: 2
  
  n_updates_jit: 4

  evaluate_every_epochs: 40000
  num_test_rollouts: 20
  video_every_epochs: 80000 
